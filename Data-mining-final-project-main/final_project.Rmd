---
title: "How to find a good paying job in the AI/ML and Big Data space in 2023"
author:
- Bibiloni Fontirroig, Josep Antoni
- Comellas Fluxá, Cristian
- Crespí Valero, Maribel
- Fortes Domínguez, Odilo
- Meneses Magon, Jhonier Duvan
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
    number_sections: yes
    fig_width: 7
    fig_height: 6
    fig_caption: yes
    highlight: tango
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      toc_collapsed: yes
    number_sections: yes
    theme: united
  word_document:
    toc: yes
    toc_depth: '3'
subtitle: |
    | Data Mining Final Project
    | 
    | GROUP 3 (ADA LOVELACE)  
always_allow_html: yes
urlcolor: blue
linkcolor: red
---


```{r setup, include=FALSE}
# Set chunk options
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, size='scriptsize')

# Set seed to replicate results
set.seed(12345)


# Loading all the libraries (if they are not installed they are first installed)
for(pkg in c("leaflet", "dplyr", "caTools", "e1071", "caret", "class", "kknn", "htmltools", "ggplot2", "GGally","xgboost","purrr","randomForest", "party", "ggpubr", "stringr", "tools", "skimr", "clustMixType", "tidyverse","import", 
             "kableExtra")){
  
  if(!require(pkg, character.only = TRUE)){
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}


def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

```

\newpage

# Introduction

This project aims to discover the factors or patterns that can help find a well-paying job in the field of Artificial Intelligence, Machine Learning and Big Data. It is focused on extracting information from the past year, which is considered the best approximation for the next year, in which you want to find a job. Specifically, it seeks to discover how to find a full-time job.

To do so, different Data Mining techniques will be used, including both supervised and unsupervised learning. These cover classification techniques such as Decision Trees and Naive Bayes. Clustering techniques, such as k-modes, and Association rules with Apriori method, are also employed. All these methods will help to answer a series of questions, the answers to which will provide us with the necessary information to obtain the knowledge we are looking for.

# Data analysis

In this section, we will analyze the data and make the necessary cleaning and changes to prepare it for the methods we will use later.

## First glance

In first place, we read the data converting strings to factors and then we check if it is properly loaded. We also inspect now the structure of the dataframe.

```{r,echo=FALSE}
data = read.csv("./AI_MLsalaries.csv", header = TRUE, stringsAsFactors = TRUE)
data_back = data
str(data, strict.width="cut")
```

There are a total of 1332 observations and 11 variables in the initial dataset. We can see that most of the variables are factors. In fact, the only numerical variables are the salary ones.

## Data and features selection

We are interested in extracting knowledge from the most recent year, since we assume that it is the one that will most resemble 2023. Therefore, we remove data from the remaining years. We can do this because most of the observations are from 2022. In addition, we want to find full-time jobs, so we can also remove samples with other types of employment. Again, we can do this since most of the jobs in the dataframe are full-time. Because of these deletions, the variables `word_year` and `employment_type` only have one value, so we can remove them. Furthermore, as far as wages are concerned, we are only interested in the variable `salary_in_usd` since we need salary to be comparable regardless of the currency in which they are paid. For this reason, we get rid of `salary`.

```{r, include=FALSE}
data = data_back
data = data[data$work_year == 2022 & data$employment_type == 'FT', -c(1,3,5)]
head(data)
```

## Cleansing and pre-processing

First, we have decided to convert the remote ratio variable to a factor with order, since it behaves logically as such, but has a discrete numerical type. It will have the values "in-place", "partial" and "remote".

Then, we realized that the `job_title` variable had different values that meant the same thing, for example, "ML Engineer" and "Machine Learning Developer". Because of this, we had to do a manual filtering, merging this kind of groups into a single value. In the case of the example, both values end up being "Machine Learning Engineer".

```{r, include=FALSE}
n_out = length(boxplot(data$salary_in_usd, plot = F)$out)
n_out
```

We also discovered that there are a total of `r n_out` outliers in the `salary_in_usd` variable, although we will not eliminate them because we are interested in discovering what factors allow us to obtain such high wages.

```{r, include=FALSE}
# Transform remote_ratio to a factor
data$remote_ratio = as.factor(data$remote_ratio)
levels(data$remote_ratio) = c("in-place", "partial", "remote")
data$remote_ratio = factor(data$remote_ratio, order=TRUE, levels=c("in-place", "partial", "remote"))
```

```{r, include=FALSE}

# Clean job titles
change_job = function(job_title){
  if(job_title %in% c("ML Engineer","Machine Learning Developer","Machine Learning Scientist")){
    return("Machine Learning Engineer")
  }
  if(job_title %in% c("Data Science Engineer")){
    return("Data Scientist")
  }
  if(job_title %in% c("Data Analytics Engineer","Analytics Engineer")){
    return("Data Analyst")
  }
  if(job_title %in% c("BI Data Analyst","Business Data Analyst")){
    return("BI Analyst")
  }
  if(job_title %in% c("Data Scientist Lead")){
    return("Data Science Lead")
  }
    if(job_title %in% c("Computer Vision Software Engineer")){
    return("Computer Vision Engineer")
  }
  return(format(job_title))
}

data$job_title_clean =as.factor(sapply(data$job_title, FUN=change_job))
data$job_title = NULL
unique(data$job_title_clean)
```

## Feature addition

The first variable we add is the salary converted to an ordered factor. This will group the salary into three, the salaries below the first quartile ("LOW" is less than 90,000), those between the first and third ("MEDIUM") and those above the third quartile ("HIGH" is more than 172,500).

Second, we will group the different job titles according to their purpose. The new variable will have a total of six values: Advanced Analytics (AA), Data Engineering (DE), Data Analytics (DA) and their leaders, AA_lead, DE_lead and DA_lead.

Then, we added four variables containing the latitude and longitude of the employees' residence and the location of the companies. These data are extracted from [another Kaggle dataset](https://www.kaggle.com/datasets/paultimothymooney/latitude-and-longitude-for-every-country-and-state) containing latitude and longitude by country code. We found that we have a sample with a code "AX" that is not found in the dataset, so we enter the data concerning this country manually. Due to the fact that we are replacing each factor by a concrete value, this variable can be considered as a factor logically, with the advantage that distances can be easily calculated between them. Related with that, we will add a binary variable that will indicate whether a worker works where he/she resides or not.

Finally, we will add a variable containing the salary adjusted for the cost of living in the employee's country of residence. This cost of living is from 2022 and it has been extracted from [Numbeo](https://www.numbeo.com/cost-of-living).

```{r, include=FALSE}

# Salary to factor
quartiles = quantile(data$salary_in_usd, probs = c(0.25, 0.75))

print(quartiles)

assing_factor = function(salary){
  if(salary <= quartiles[1]){
    return("LOW")
  }
  if(salary >= quartiles[2]){
    return("HIGH")
  }
  return("MEDIUM")
}

data$salary_factor = as.factor(sapply(data$salary_in_usd, FUN=assing_factor))
data$salary_factor = factor(data$salary_factor, order=TRUE, levels=c("LOW", "MEDIUM", "HIGH"))
data$salary_factor
```

```{r, include=FALSE}

# Grouping jobs

group_job = function(job_title){
  if(job_title %in% c("ML Engineer","Machine Learning Developer","Machine Learning Engineer","Data Scientist","Applied Data Scientist","Data Science Consultant","Machine Learning Scientist","Applied Scientist","AI Scientist","Applied Machine Learning Scientist","NLP Engineer","Product Data Scientist","Principal Data Scientist","Research Scientist","Computer Vision Engineer","3D Computer Vision Researcher","Machine Learning Infrastructure Engineer")){
    return("AA")
  }
  if(job_title %in% c("Data Science Lead","Machine Learning Manager","Lead Data Scientist","Lead Machine Learning Engineer","Director of Data Science","Data Science Manager","Data Scientist Lead","Head of Data Science","Head of Machine Learning")){
    return("AA_lead")
  }
  if(job_title %in% c("Data Engineer","ETL Developer","Big Data Engineer","Principal Data Architect","Data Architect","Research Engineer","Data Specialist","Data Operations Engineer")){
    return("DE")
  }
  if(job_title %in% c("Data Engineering Manager","Lead Data Engineer","Data Manager","Head of Data")){
    return("DE_lead")
  }
  if(job_title %in% c("Data Analytics Engineer","Analytics Engineer","Marketing Data Analyst","Data Analyst","BI Analyst","Business Data Analyst","Data Analytics Consultant","Product Data Analyst","Principal Data Analyst","Financial Data Analyst","Financial Data Analyst","Data Operations Analyst")){
    return("DA")
  }
    if(job_title %in% c("Data Analytics Manager","Data Analytics Lead")){
    return("DA_lead")
  }

  return(format(job_title))
}

data$job_title_sector = as.factor(sapply(data$job_title_clean, FUN=group_job))
table(data$job_title_sector)

```

```{r, include=FALSE}

# Getting coordinates

# Read dataframe containing data of the countries
countries_info = read.csv("world_country_lat_long.csv", header = TRUE)[,c("country_code", "latitude", "longitude", "country")]

# Remove NA's
countries_info = na.omit(countries_info)

new_row = data.frame(country_code="AX", latitude=60.1785247, longitude=19.91561, country="Islas de Åland")
countries_info = rbind(countries_info, new_row)

# Defining functions to get latitude, longitude and name
getLatitude = function(code){
  return(countries_info[countries_info$country_code == code, "latitude"])
}

getLongitude = function(code){
  return(countries_info[countries_info$country_code == code, "longitude"])
}

getCountryName = function(code){
  return(countries_info[countries_info$country_code == code, "country"])
}

# Add columns of latitude and longitude for each location
data$employee_residence_latitude = sapply(data$employee_residence, FUN = getLatitude)
data$employee_residence_longitude = sapply(data$employee_residence, FUN = getLongitude)
data$company_location_latitude = sapply(data$company_location, FUN = getLatitude)
data$company_location_longitude = sapply(data$company_location, FUN = getLongitude)

```

```{r, include=FALSE}
living_index = read.csv(file="Cost_of_Living_Index_2022.csv", header = T)

getLivingIndex = function(code){
  
  # special case
  if(code == "AX"){
    return(68.97)
  }
  
  country = getCountryName(code)
  return(living_index[living_index$Country == country, "Cost.of.Living.Index"])
}

living_index_col = sapply(data$employee_residence, FUN = getLivingIndex)
data$living_index = living_index_col
data$adjusted_salary_in_usd = data[,"salary_in_usd"] / living_index_col
```



```{r, include=FALSE}
data$employee_works_where_lives = (data$employee_residence_latitude == data$company_location_latitude) & (data$employee_residence_longitude == data$company_location_longitude)
```

## Summary

Once the pre-processing is finished, we end up with `r nrow(data)` samples and `r ncol(data)` variables. We can now inspect it for strange distributions or unbalanced factors. We note that most of the factors are unbalanced, which may pose a problem for subsequent analysis. In the case of the factors with more than five levels, only the five with the highest frequency are shown, ordered from highest to lowest. On the other hand, we see that the salary follows an approximately normal distribution, although there is a tail at the end, which represents outliers with salaries around 300,000 and 400,000 USD. Moreover, since most of the data is from USA, we don't see any change on the shape of the adjusted salary distribution.

```{r,echo=FALSE, fig.width=20, fig.height=10, fig.align='center'}

format_name = function(name){
  new_name = str_replace_all(name, "usd", "USD")
  new_name = str_split(new_name, "_")[[1]]
  new_name[1] = toTitleCase(new_name[1])
  new_name = paste(new_name, collapse = " ")
  return(new_name)  
}

get_initials_if_needed <- function(name, limit=8){
    if(nchar(name) > limit){
      
      new_name = strsplit(name, split = " ")
      new_name = sapply(new_name, function(element){
        substr(element, 1, 1)
      })
      new_name = paste(new_name, collapse = "")
      return(new_name)
    }
  return(name)
}

plotDistributions <- function(df) {
  plotDistribution <- function(column) {
    
    formatted_column = format_name(column)
    
    base_plot <- ggplot(df, aes_string(x = column, fill = column)) +
      ggtitle(formatted_column) + 
      xlab(formatted_column)

    
    vec <- df[, column]
    if (is.factor(vec) || is.logical(vec)) {
      
      if(is.factor(vec) && length(levels(vec)) > 5){
      
        ordered_values = sort(table(vec), decreasing = TRUE)[1:5]
        new_levels = names(ordered_values)
        vec = factor(vec[vec %in% new_levels], levels=new_levels)
      
        
        prev_vec = vec
        levels(vec) = sapply(levels(vec), get_initials_if_needed)
        
        
        
        new_df = data.frame(vec, prev_vec)
        colnames(new_df) = c(column, paste(column, " "))
        
        base_plot <- ggplot(new_df, aes_string(x = column, fill =  paste(column, " "))) +
        ggtitle(formatted_column) + 
        xlab(formatted_column)
        
      }
      
      plot <- base_plot +
        geom_bar(stat = "count") +
        geom_text(stat = "count", aes(label = ..count..), vjust=-0.1) + 
        guides(fill=guide_legend(title=formatted_column))
    } else {
      plot <- base_plot +
        geom_density(color = "steelblue", fill = "lightblue") +
        geom_vline(aes(xintercept = mean(vec)), linetype = "dashed")
    }

    plot <- plot +
      scale_color_brewer(palette = "Dark2")

    return(plot)
  }


  plot_list <- map(names(df), plotDistribution)

  plt <- ggarrange(plotlist = plot_list, ncol = 4, nrow = ceiling(length(plot_list) / 4))

  annotate_figure(plt, top = text_grob("Distribution of the variables", size = 16, face = "bold"))
}


plotDistributions(data[,-c(6,11,12,13,14)])
```

We will review in more detail the variables related to latitude and longitude as we answer the questions.

# Questions

In this section, we will answer a series of questions to dig into the data and extract information that will be useful to gain knowledge and develop the best advice for finding a well-paying job next year (2023).

First, we are interested in determining whether the worker's location can affect his or her salary. We also want to find if there is any way to group workers more naturally from the data. In addition, we want to find associations between different factors that tell us which characteristics are more present in those workers with the highest salaries. Finally, we want to know what salary we could obtain next year with our current characteristics.

We will use different methods to answer these questions. Among these, we will employ more traditional methods such as the inspection of graphs and the study of different statistics. On the other hand, we will employ another more automated perspective, using techniques such as clustering, associative rule mining, and some classification methods, such as decision trees, k-nearest neighbors, and Naive Bayes.

## Q1: How does location influence salary?

In this section we will use basic statistics and plots interpretation to answer the question. To begin with, we will check if there is any difference between the group of those who work in the same place they reside and those who do not. We do this because we want to know if either of the two is paid more than the other, because there are few workers who reside elsewhere than where they work, and if they were paid more it could be a good indicator for high salaries.

```{r, echo=FALSE, fig.width=20, fig.height=3, fig.align='center', warning=FALSE, message=FALSE}


# --------------------- Salary

mean_plot = ggplot(data, aes(x=employee_works_where_lives, y=salary_in_usd)) +
  geom_bar(stat="summary", fun.y="mean", fill=c("#9dcebf", "#eeb990")) +
  ggtitle("Mean and standard deviation by group") + 
  ylab("Mean salary in USD") + xlab("Employee is working where he/she lives") + 
  geom_errorbar(stat="summary", 
                fun.y = "mean",
                fun.ymin=function(x) {mean(x)-sd(x)/sqrt(length(x))}, 
                fun.ymax=function(x) {mean(x)+sd(x)/sqrt(length(x))}) +
  geom_text(stat = "summary", fun = "mean",
            aes(y = stage(salary_in_usd, after_stat = y / 2),
                label = round(after_stat(y), 2)))

my_boxplot = base = ggplot(data, aes(x=salary_in_usd, col=employee_works_where_lives)) + 
  geom_boxplot() +
  ggtitle("Distribution by group") + 
  coord_flip() +
  xlab("Salary in USD") + 
  scale_color_brewer(palette = "Dark2") 

my_boxplot <- my_boxplot + 
  guides(col=guide_legend(title="Employee works \nwhere lives"))



# --------------------- Adjusted salary

mean_plot_adjusted = ggplot(data, aes(x=employee_works_where_lives, y=adjusted_salary_in_usd)) +
  geom_bar(stat="summary", fun.y="mean", fill=c("#9dcebf", "#eeb990")) +
  ggtitle("Mean and standard deviation by group") + 
  ylab("Mean adjusted salary in USD") + xlab("Employee is working where he/she lives") + 
  geom_errorbar(stat="summary", 
                fun.y = "mean",
                fun.ymin=function(x) {mean(x)-sd(x)/sqrt(length(x))}, 
                fun.ymax=function(x) {mean(x)+sd(x)/sqrt(length(x))}) +
  geom_text(stat = "summary", fun = "mean",
            aes(y = stage(adjusted_salary_in_usd, after_stat = y / 2),
                label = round(after_stat(y), 2)))

my_boxplot_adjusted = base = ggplot(data, aes(x=adjusted_salary_in_usd, col=employee_works_where_lives)) + 
  geom_boxplot() +
  ggtitle("Distribution by group") + 
  coord_flip() +
  xlab("Adjusted salary in USD") + 
  scale_color_brewer(palette = "Dark2") 

my_boxplot_adjusted <- my_boxplot_adjusted + 
  guides(col=guide_legend(title="Employee works \nwhere lives"))



# ------------------ Put them together


plt <- ggarrange(mean_plot, my_boxplot, mean_plot_adjusted, my_boxplot_adjusted, ncol = 4, nrow = 1)

annotate_figure(plt, top = text_grob("Salary depending on whether the employee works and resides in the same location         Adjusted salary depending on whether the employee works and resides in the same location ", size = 16, face = "bold"))
```

We observe that there is a clear difference between the salary in the group of those who reside in the same place where they work and those who do not, with the latter set of workers earning the least. We see this because the mean of the different groups does not fall within the range established by the mean plus minus the standard deviation of the other group. Furthermore, looking at the boxplot, we see that practically 100% of the samples are below 75% of those working where they reside, except for two outliers.

This fact could be due to the fact that employees working abroad are living in places with a lower cost of living, so the salary is regulated according to this factor. To check this, the same graphs have been shown with the adjusted salary. In this case, we observe that there are no longer significant differences between the two groups, indicating that salaries are adjusted according to the cost of living. Because of this, we see that whether you work where you live or not has no real impact on purchasing power. 

```{r, include=FALSE}
zoom = TRUE
point_radius = 4
sd = 3
if(knitr::is_latex_output()){
  zoom = FALSE
  knitr::opts_chunk$set(fig.width=5.5, fig.height=1.5)
  point_radius = 1
  sd = 2
}
```


```{r, echo=FALSE, message=FALSE, fig.align='center'}


# Map of employee residence
options(viewer = NULL)

xnoise = rnorm(data$salary_in_usd, mean = 0, sd = sd)
ynoise = rnorm(data$salary_in_usd, mean = 0, sd = sd)

pal <- colorNumeric(c("blue", "red"), domain = data$salary_in_usd)
colors <- pal(data$salary_in_usd)

title1=tags$div(
  HTML("<h4>Employee<br>Residence</h4>")
  )

m1 <- leaflet(options = leafletOptions(zoomControl=zoom, attributionControl=FALSE)) %>%
  addTiles() %>%
  addProviderTiles("Esri.OceanBasemap") %>%
  addCircleMarkers(
    lng = data$employee_residence_longitude+xnoise,
    lat = data$employee_residence_latitude+ynoise,
    radius = point_radius,
    color = colors,
    opacity = 0.5,
    popup = paste("Salary: ", as.character(data$salary_in_usd))
  ) %>%
  addLegend("topright",
    pal = pal,
    values = data$salary_in_usd,
    title = "Salary",
    opacity = 1,
    labFormat = labelFormat(prefix = "US$ "),
    bins = 5
  ) %>%
  addControl(title1, position = "topleft")

m1
```

```{r, include=FALSE}


xnoise = rnorm(data$adjusted_salary_in_usd, mean = 0, sd = sd)
ynoise = rnorm(data$adjusted_salary_in_usd, mean = 0, sd = sd)

pal <- colorNumeric(c("blue", "red"), domain = data$adjusted_salary_in_usd)
colors <- pal(data$adjusted_salary_in_usd)

title1=tags$div(
  HTML("<h4>Employee<br>Residence</h4>")
  )

m2 <- leaflet(options = leafletOptions(zoomControl=zoom, attributionControl=FALSE)) %>%
  addTiles() %>%
  addProviderTiles("Esri.OceanBasemap") %>%
  addCircleMarkers(
    lng = data$employee_residence_longitude+xnoise,
    lat = data$employee_residence_latitude+ynoise,
    radius = point_radius,
    color = colors,
    opacity = 0.5,
    popup = paste("Salary: ", as.character(data$adjusted_salary_in_usd))
  ) %>%
  addLegend("topright",
    pal = pal,
    values = data$adjusted_salary_in_usd,
    title = "Salary",
    opacity = 1,
    labFormat = labelFormat(prefix = "US$ "),
    bins = 5
  ) %>%
  addControl(title1, position = "topleft")

m2
```

We will now look at salaries in different countries using a [Leaflet map](https://rstudio.github.io/leaflet/). To better see the salaries, some normal noise has been added to the coordinates of each sample so that all the points do not overlap in one place. In addition, no differences were observed in the case of adjusting wages to the cost of living, so only the wage in USD is shown. The map shows that the highest salaries are in the United States. Good salaries are also found in Canada and Great Britain. In the rest of the world, some high salaries can be found in localized areas, such as Nigeria, but we have too few samples from certain regions to draw conclusions. 

According to the results obtained in this section, in general, it is advisable to work for a company in the united states if you want to obtain a salary well above the average. It is possible to work from the same country or from abroad, but it must be remembered that the salary will be adjusted to the cost of living in the country of residence.

## Q2: Can we cluster data to discover any patterns?

Clustering algorithms are specifically used to discover patterns and relationships within the data that may not be obvious from a simple visual inspection of the raw data. The application of this algorithm may result in a better understanding of the data. The results will then allow us to add a new (categorical) feature to the dataset, where for each row it will indicate the obtained cluster it belongs to.

First of all, we will have to carefully select the features to work with, as with too many features the clustering algorithm may have difficulties finding meaningful patterns.
```{r, include=FALSE}
summary(data)
```
We will delete all coordinates related variables except the company ones (as most workers live in the same country they work at), all salary related except salary_in_usd because it is the one that gives us the most information of all, and job_title_clean because it is too diverse.
```{r, include=FALSE}
data2 <- data %>% 
         dplyr::select(-c("employee_works_where_lives","salary_factor",
                          "salary_currency","job_title_clean",
                          "employee_residence","company_location",
                          "employee_residence_latitude","employee_residence_longitude")) %>% 
         relocate(salary_in_usd,.before=experience_level) %>%
         relocate(adjusted_salary_in_usd,.before=experience_level) %>%
         as_tibble()
```
The variables type is a key factor that affects which algorithms can be applied. In this case, we have left a similar quantity of categorical and numerical features (4 and 3 respectively) so we have decided to use k-prototype as it is specifically designed to handle mixed data types. It will allow us to cluster by taking into account the unique characteristics of both feature types.

The next step we need to do is select the optimal number of clusters (k), and to do so we can use the elbow method. The idea behind the elbow method is to plot the explained variation as a function of the number of clusters, and pick the elbow of the curve as the number of clusters to use. The plot for optimum k will be shown at the end of the section. If we take a look at the elbow it can be seen that we could either pick 4 or 5 as our number of clusters, because it represents the point where the increase in number of clusters does not yield much improvement in terms of WCSS or variance explained and avoids overfitting. In this case we will pick 4, but 5 would have been a good election as well. We will then execute the algorithm setting k=4.

```{r, include=FALSE}
set.seed(123456)

k.max <- 15
wss <- sapply(1:k.max, 
              function(k){kproto(data2[,c(-1, -2)], k)$tot.withinss})
```


```{r, results='hide'}
optimum <- 4; set.seed(42); kpres <- kproto(data2[,c(-1, -2)], optimum, verbose = FALSE); kpres;
```

Now we will review the results. We know that the Euclidean distance is used to calculate the similarity between observations. The results show that the the number of observations of the clusters are 143, 203, 647 and 26. In addition, the within cluster error is 415109, 328329, 1208270 and 122392, respectively. This indicates how similar the observations are within each cluster. Lower values of this metric mean that the observations within a cluster are more similar.

Based on the within cluster error values provided, it can be concluded that the clustering algorithm has done a relatively good job of grouping similar data points together. We can observe that cluster 2 has a low error for its size so its members must be pretty similar, cluster 3 has a very high error because it has a lot of members, and cluster 4 has a high error despite of it having very few members.
```{r, include=FALSE}
summary(kpres)
```
Now let's add the cluster column to the dataset with the new feature, then visualize and inspect the data. We will also plot the distribution of the salary in USD for every cluster to analyze them.



```{r, include=FALSE}
# New dataset with the feature of clusters.
data_cl <- tibble(data2,cluster=factor(kpres$cluster, order =  TRUE,
                                       levels = c(1:optimum)))
tail(data_cl)
```

```{r, echo=FALSE}
# General summary of grouped clusters.
result <- data.frame(kpres$size, kpres$centers) %>% 
          mutate(clusters=as.numeric(clusters)) %>% 
          left_join(
            data_cl %>% 
              group_by(clusters=cluster) %>%
              summarise(salary_in_usd=mean(salary_in_usd)) %>% 
              ungroup() %>% 
              mutate(clusters=as.numeric(clusters)),
            by=c("clusters")) %>%
          as.data.frame() 
          
knitr::kable(result, booktabs = T, escape = F, col.names = sapply(colnames(result), FUN=format_name), align="c") %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"), position="center")
  
```

Overall we can see that cluster 3 earns significantly more than the rest (they earn US\$ 150k aprox). Cluster 2 earns close to US\$ 40k  less than cluster 3 despite the only differences being that cluster 2 sector is data analysis while cluster 3 is advanced analytics. We can also observe that not living in the US makes a big impact to the income (clusters 1,4 vs clusters 2,3), taking into account that samples from cluster 1 are closer to Europe and samples from cluster 4 are closer to Asia.

This adds to the fact that to get a good paying job in this sector we should try to aim for a job in the US, specifically in the advanced analytics sector.

```{r, include=FALSE}
if(knitr::is_latex_output()){
  knitr::opts_chunk$set(fig.height=3)
}
```


```{r, fig.align='center', fig.width=10, echo=FALSE}

par(mfrow=c(1,2))

plot(1:k.max, wss, col="blue", lty=2, lwd=1,
     type="b", pch = 19, frame = TRUE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares",
     main="Evaluation of the optimal number of clusters")

# Evaluation of difference of salaries per cluster.
boxplot(data_cl$salary_in_usd~as.character(data_cl$cluster),
        col=2:5,main="Salaries per cluster",xlab="Cluster",ylab="Salary in USD")
```


## Q3: Which factors help the most to get high salaries?

In the initial stage of our Apriori analysis, it is necessary to transform the original dataset into a transactional format. This involves converting any non-factor values, such as salary, into categorical variables. In this case, we will categorize salaries into three distinct groups: HIGH, MEDIUM, and LOW. The specific salary values themselves are not of interest in this analysis.

```{r, include=FALSE}
library(dplyr)
library(arules)
data_apriori<- data %>% select(-salary_in_usd,
                               -employee_residence_latitude, -employee_residence_longitude, -company_location_latitude, -company_location_longitude)
write.csv(data_apriori, "data_for_apriori.csv")
dataset = read.transactions("./data_for_apriori.csv", sep = ',', rm.duplicates = TRUE)
dataset = dataset[-c(1), ]
summary(dataset)

```

Our objective is to secure top-paying positions in the field of Machine Learning and AI. To this end, we have conducted an analysis to uncover the factors that play a role in determining high salaries in this industry. The results of this analysis provide valuable insights for those seeking to secure high-paying positions in the field of Machine Learning and AI.

```{r, eval=FALSE}
rules_high_salary = apriori(data = dataset, 
          parameter = list(support = 0.001, #Support small value since you think the data set is relatively large.
          confidence = 0.7,target = "rules"), #Confidence value of 0.7 for strong rules.
          appearance = list(rhs = "HIGH")) #In rhs we are interested in the "HIGH" value of salary.
 inspect(sort(rules_high_salary[size(rules_high_salary) > 6], by = 'lift')[1:15])
 # We use is.maximal since we want only the most general rules.
 reglas_maximales <- rules_high_salary[is.maximal(rules_high_salary)][1:7] 
```

In general, it appears that remote jobs and companies located in the US that pay in USD are the most common factors associated with high salaries. It is also important to note that face-to-face jobs can also offer high salaries.
When it comes to the most well-paying jobs, the positions of Applied Data Scientist within the AA context, and large companies, as well as having senior experience, are key factors.
   
```{r, include=FALSE}
# Next, we will define the association rules to determine the factors that influence high salaries in small, medium, and large companies, respectively.
rules_high_salary = apriori(data = dataset, 
                            parameter = list(support = 0.001,
                             confidence = 0.7,target = "rules"),
                            appearance = list(rhs = "HIGH"))

# We check which rules usually go together according to the size of the company. We check each subset where the size of the company is found in the lhs. We do it for each possibility, that is, SMALL, MEDIUM and LARGE.
 high_SMALL <- subset(x = rules_high_salary,subset = lhs %ain% c("S"))
inspect(sort(high_SMALL , by = 'lift')[1:15])
high_MEDIUM <- subset(x = rules_high_salary, subset = lhs %ain% c("M"))
inspect(sort(high_MEDIUM , by = 'lift')[1:15])
high_LARGE <- subset(x = rules_high_salary, subset = lhs %ain% c("L"))
inspect(sort(high_LARGE, by = 'lift')[1:15])


```

An analysis of the factors influencing high salaries in small, medium and large companies respectively shows differences in job titles such as Applied Data Scientist, Machine Learning Scientist, Machine Learning Engineer, DE_lead and Data Science Manager.

Regardless of company size, experience and seniority, location in the US and payment in USD appear to be important factors. The highest paying sector in small and large companies is AA, while DE_lead is the most prominent sector in medium-sized companies. No significant difference is observed between remote and face-to-face work.

For smaller companies, the highest paying jobs are Machine Learning Scientist and Machine Learning Engineer, for medium-sized companies it is Data Science Manager and for large companies it is Applied Data Scientist.

```{r, include=FALSE}

#We will also check what factors differ if I want to get a high salary, in face-to-face work and in remote work.

rules_high_salary = apriori(data = dataset, 
                            parameter = list(support = 0.001,
                             confidence = 0.7,target = "rules"),
                            appearance = list(rhs = "HIGH"))

# The idea is the same as in the previous section, but depending on whether the work is done remotely or in person.
high_presential_results <- subset(x = rules_high_salary,subset = lhs %ain% c("in-place"))
inspect(sort(high_presential_results , by = 'lift')[1:15])
high_remote_results <- subset(x = rules_high_salary, subset = lhs %ain% c("remote"))
inspect(sort(high_remote_results, by = 'lift')[1:15])
```

Finally, an analysis of factors affecting high salaries in both remote and face-to-face jobs shows that senior experience, location in the US and payment in USD are important factors. The highest paying remote jobs are Applied Data Scientist, AI Scientist and Machine Learning Engineer/Scientist, while the highest paying face-to-face job is Data Science Manager in the AA_lead sector.


## Q4: What are the most influential characteristics?

The process uses the salary dataset, which has information on job type, work experience, geographic location, etc., to train a decision tree model. The model finds patterns and relationships in the data to identify key factors in predicting AI/ML and Big Data salaries.

```{r, echo=FALSE}
set.seed(1254)
df <- data[,c("salary_factor","experience_level", "living_index","job_title_clean","employee_residence","remote_ratio","company_location","company_size","job_title_sector")]
indice_entrenamiento <- sample(1:nrow(df), 0.75*nrow(df))
train <- df[indice_entrenamiento,]
test <- df[-indice_entrenamiento,]
```

We will use the ctree algorithm from the party package in R, which builds a robust, reliable conditional inference tree based on non-parametric tests. It is effective for datasets with many predictors, and can handle continuous and categorical variables, making it suitable for analyzing AI/ML and Big Data salaries.


```{r, echo=FALSE}
# Entrenar el ctree
ctree_model <- ctree(salary_factor~., data = train, controls = ctree_control(mincriterion = 0.95,minsplit = 100,minbucket = 5))
print(ctree_model)
```


After training a decision tree, the most important factors are residence (company location and employee location are practically the same), experience, job sector, and living index. Location is considered the top factor, followed by experience and job sector. The importance of each feature may vary with the dataset and problem at hand. It is essential to remember that the feature importance may change depending on the dataset used to train the decision tree.
If we plot the decision tree, we can see that to increase salary, the company must be located in the US, UAE or Malaysia, Puerto Rico and Nigeria (which are outliers), and the candidate must have strong experience in AA or DA. Lower paying jobs are in other countries with limited experience and a living index below 68.97.

```{r,echo=FALSE}
pred = predict(ctree_model, test)
cm = confusionMatrix(test$salary_factor, pred)
```

```{r, echo=FALSE, fig.width=12, fig.height=6, fig.align='center'}
# Verificar el modelo
plot(ctree_model)
```

The accuracy of the decision tree, which is `r round(cm$overall[1] * 100, 2)`%, representing the proportion of correct predictions made by the model. Accuracy can be helpful, but it may not fully reflect a model's performance, especially for imbalanced datasets. 

It has been noted that the location of the company or employee has the most impact on salary, followed by work experience, job type and living index. In particular, the country where the company is located greatly affects salary.

## Q5: Based on our characteristics, what salaries would we have?

The objective of this question is to train a model that allows us to know in which salary range a worker is. We can use this model later to predict ourselves and get an idea of the salary we could have based on our characteristics.

First, we will select the characteristics of the model. We will remove from the previous dataset `salary_in_usd` and `adjusted_salary_in_usd`, since it is a classification model and therefore we will use "salary_factor" as target.

We will also remove `company_location`, `company_location_longitude`, `company_location_latitude`, 
`employee_works_where_lives` and `employee_residence` since the `employee_residence_latitude` and 
`employee_residence_longitude` variables provide the same information. Finally we will scale the numerical data of the dataset.

```{r,include=FALSE}
data_model <- data[ , -which(names(data) %in% c("salary_in_usd", "company_location","employee_residence","adjusted_salary_in_usd","employee_works_where_lives","company_location_longitude","company_location_latitude"))]
data_model = mutate_if(data_model, is.numeric, scale, center = FALSE)
```

```{r,include=FALSE}
set.seed(12)
split = sample.split(data_model, 0.8)
train = subset(data_model, split == TRUE,seed = 123)
test = subset(data_model, split == FALSE,seed = 123)
```


### Model selection

We are going to select the classification model. To do so, we are going to split the data into 80% train and 20% test. We will train different models and choose the one that gives us the best results. It should be noted that the models reviewed in class such as knn and Naive Bayes do not give us good results, so we will use other unseen models apart from these.

Let's see the models that we trained. We have used **Naive Bayes** with the default hyperparameters and **k-Nearest Neighbors** (with kmax=5). We have also used other unseen models that we will briefly explain below.

```{r,include=FALSE}
set.seed(123)
model.NB <- naiveBayes(salary_factor ~ . ,data = train)
model_pred.NB<-predict(model.NB,test)
```

```{r,include=FALSE}
table.NB = table(test$salary_factor,model_pred.NB)
precision.NB <- (sum(diag(table.NB)))/sum(table.NB)
precision.NB
```
```{r,include=FALSE}
set.seed(456)
model.knn <- train.kknn(train$salary_factor ~ ., data = train, kmax =5)
model_pred.knn = predict(model.knn, test)
```

```{r,include=FALSE}
table.knn = table(test$salary_factor,model_pred.knn)
precision.knn <- (sum(diag(table.knn)))/sum(table.knn)
precision.knn
```
* **Support Vector Machines** (kernel=linear, cost=1): it is a supervised machine learning algorithm used for classification and regression problems. It aims to find the best boundary between classes by maximizing the margin between classes, which is the distance between the boundary and closest data points of each class. The closest data points are known as support vectors and have the greatest impact on the boundary. SVM can handle non-linearly separable data by transforming them into a higher dimensional space mediante un kernel  where a linear boundary can be found. 

```{r,include=FALSE}
set.seed(123)
model.svm <- svm(train$salary_factor ~ ., data = train, kernel = "linear",seed = 123)
model_pred.svm <- predict(model.svm, test)
```

```{r,include=FALSE}
table.svm = table(test$salary_factor,model_pred.svm)
precision.svm <- (sum(diag(table.svm)))/sum(table.svm)
precision.svm
```

* **Random Forest** (default): this is a supervised learning method for classification and regression, which operates by building a multitude of decision trees at training time. The trees are built using self-starting data samples and a random subset of features for each split, which increases the randomness of the model. Combining many trees in a random forest leads to a decrease in overfitting and an increase in precision compared to a single decision tree.

```{r,include=FALSE}
#for more info abaut RF see:https://towardsdatascience.com/understanding-random-forest-58381e0602d2
model.rf <-  randomForest(train$salary_factor ~ ., data = train, seed = 123)

model_pred.rf <- predict(model.rf, test)
```

```{r,include=FALSE}
table.rf = table(test$salary_factor,model_pred.rf)
precision.rf <- (sum(diag(table.rf)))/sum(table.rf)
precision.rf
```

* **XGBoost** (objective="multi:softmax, booster="gbtree"): XGBoost is a library that implements gradient boosting algorithm on decision trees and has been used in various real-world applications. XGBoost is known for its speed and performance and often used for large-scale data science projects.
```{r,include=FALSE}
# objective="multi:softmax is used to classify
# xgboost suport in a lot lenguages(python,r...) and framework like SPARK . See more: shttps://xgboost.readthedocs.io/en/stable/
```



```{r,include=FALSE}

data_xgb=data_model
data_xgb <- map_df(data_xgb, function(columna) {
  columna %>% 
    as.factor() %>% 
    as.numeric %>% 
    { . - 1 }
})
```

```{r,include=FALSE}
set.seed(123)
train_df <- sample_frac(data_xgb, size = 0.7)
test_df <-  setdiff(data_xgb,train_df)
```

```{r,include=FALSE}
train_matrix <- 
  train_df %>% 
  select(-salary_factor) %>% 
  as.matrix() %>% 
  xgb.DMatrix(data = ., label = train_df$salary_factor)
```

```{r,include=FALSE}
test_matrix<- 
  test_df %>% 
  select(-salary_factor) %>% 
  as.matrix() %>% 
  xgb.DMatrix(data = ., label = test_df$salary_factor)
```

```{r,include=FALSE}
model.xgboost <- xgboost(data = train_matrix,booster="gbtree",objective = "multi:softmax",                                num_class = 3, sampling_method="uniform", nrounds = 140,                                         max.depth = 4,eta =0.1, nthread=2, max_delta_step=1,gamma=0.1,verbose=0)
```

```{r,include=FALSE}
model_pred.xgboost <- predict(model.xgboost,test_matrix)
```

```{r,include=FALSE}
table.xgboost = table(test_df$salary_factor,model_pred.xgboost)
precision.xgboost <- (sum(diag(table.xgboost)))/sum(table.xgboost)
precision.xgboost
```

```{r,include=FALSE}
error.NB=1-precision.NB
error.knn=1-precision.knn
error.svm=1-precision.svm
error.rf=1-precision.rf
error.xgboost=1-precision.xgboost
```


Let's see the results of each model:
```{r,echo=FALSE}
metric <- c("precision", "error")
naiveBayes<- c(precision.NB,error.NB)
k_NN <- c(precision.knn,error.knn)
linear_svm<-c(precision.svm,error.svm)
randomForest<- c(precision.rf,error.rf)
xgboost <-c(precision.xgboost,error.xgboost)
metric_data <- data.frame(metric,naiveBayes,k_NN,linear_svm,randomForest,xgboost)
metric_data = mutate_if(metric_data, is.numeric, round,3)


knitr::kable(metric_data, booktabs = T, escape = F, col.names = c("Metric", "Naive Bayes", "K-NN", "Linear SVM", "Random Forest", "XGBoost"), 
             align=c("l", rep("c", ncol(metric_data) - 1))) %>%
  kable_styling(latex_options = c("striped","scale_down"), full_width = T) %>%
  column_spec(1, bold=T)
```
We can see that KNN and Naive Bayes do not give us good results. Xgboost is more complex and its results are regular for having a small dataset. SVM and random forest give us better results. We will select random forest as it is the best.

### Validate model and our predictions

```{r,include=FALSE}
# use cv for check model
cv_results.RF <- caret::trainControl(method = "cv", number = 10)
cv_model.RF <- train(salary_factor ~ ., data = train, method = "rf", trControl = cv_results.RF)
```


```{r,include=FALSE}
cv_model_pred.RF <- predict(cv_model.RF, newdata = test)
cv_table.RF = table(test$salary_factor,cv_model_pred.RF)
cv_precision.RF <- (sum(diag(cv_table.RF)))/sum(cv_table.RF)
cv_precision.RF
```

```{r,include=FALSE}
model.rf <-  randomForest(train$salary_factor ~ ., data = train,mtry=75, seed = 123)

model_pred.rf <- predict(model.rf, test)
```

```{r, include=FALSE}
table.rf = table(test$salary_factor,model_pred.rf)
precision.rf <- (sum(diag(table.rf)))/sum(table.rf)
precision.rf
```
Once the model is selected, to give more robustness to the metrics, we will use k-fold cross -validation with k=10. It gives us a similar result (`r round(cv_precision.RF * 100, 2)`% precision), therefore it is not a coincidence. The final model used to predict gives us a precision of `r round(precision.rf*100, 2)`%.

Finally, we are going to use the model to predict ourselves:

```{r, include=FALSE}

create_prediction_row = function(exp_level, salary_currency, employee_residence, remote_ratio, company_size, job_title){
  return(
    data.frame(
      experience_level = factor(exp_level, levels=levels(data$experience_level)),
      salary_currency = factor(salary_currency, levels=levels(data$salary_currency)),
      remote_ratio = factor(remote_ratio,ordered = TRUE, levels=levels(data$remote_ratio)),
      company_size = factor(company_size, levels=levels(data$company_size)),
      job_title_clean = factor(job_title, levels=levels(data$job_title_clean)),
      job_title_sector = factor(group_job(job_title), levels=levels(data$job_title_sector)),
      employee_residence_latitude = getLatitude(employee_residence),
      employee_residence_longitude = getLongitude(employee_residence),
      living_index = getLivingIndex(employee_residence)
    )
  )
}

cristian <- create_prediction_row("MI", "USD", "US", "partial", "M", "Machine Learning Engineer")
peptoni <- create_prediction_row("MI", "EUR", "ES", "partial", "L", "Machine Learning Engineer")
odilo <- create_prediction_row("MI", "USD", "US", "in-place", "M", "Machine Learning Engineer")
maribel <- create_prediction_row("MI", "GBP", "GB", "remote", "L", "Data Scientist")
jhonier <- create_prediction_row("MI", "EUR", "IE", "partial", "L", "Data Engineer")

data_to_predict <- rbind.data.frame(cristian, peptoni, odilo, maribel, jhonier)
data_to_predict <- mutate_if(data_to_predict, is.numeric, scale, center = FALSE)
predicted_salary_rank <- predict(model.rf, newdata = data_to_predict)
predicted_salary_rank
data_to_predict$salary_predicted = predicted_salary_rank
data_to_predict$name= c("Cristian","Pep Toni","Odilo","Maribel","Jhonier")
data_to_predict
```

```{r, echo=FALSE}
#pivot_df = pivot_wider(names_from = name, values_from = salary_predicted, data= data_to_predict[,c("name","salary_predicted")])

variables = c("name","experience_level", "salary_currency","remote_ratio","job_title_clean", "company_size","salary_predicted")

knitr::kable(data_to_predict[,variables], booktabs = T, escape = F, col.names = sapply(variables, FUN=format_name), 
             align=c(rep("c", length(variables)-1), "r")) %>%
  kable_styling(latex_options = c("striped","scale_down"), full_width = T) %>%
  column_spec(length(variables) , bold=T) %>%
  column_spec(5, width="4cm")
```


# Conclusion
Having answered the above questions, we have gained the necessary knowledge to build a guide on how to get a good paying job in 2023 in AI/ML and Data Science space. 

Generally, salary is largely influenced by a company's or employee location, with factors like work experience, job type, and cost of living also playing a role. The country where the company is based has a big impact on salary, with higher salaries typically seen in companies based in the US or UAE. To boost pay, a candidate should target jobs in these countries and have extensive experience in Advanced Analytics (AA) or Data Analytics (DA).

In particular, we know that it is beneficial to learn English, since the best way to get a higher salary is to work for a company in the United States or other English-speaking areas such as Canada and Great Britain. In general, both remote and in-place jobs are well paid, but there are higher salaries on remote, so it could be convenient to find a remote job. Even so, it is important to take into account the cost of living in the country where you reside, as it will affect your remuneration. Regarding the type of work, the AA sector has the highest salaries, with Applied Data Scientist and Machine Learning Engineer being common roles, where you would be interested in working to earn more money. A curious fact that we found out is that US seniors working remote and in medium companies in the AA sector earn close to 50% more than their Data Analyst counterparts. Finally, although it is obvious, it is also important to stress the importance of gaining experience in order to get a better salary, since the salary increase between mid-level and senior level is enormous. Because of this, the salaries predicted for us by the model are low to medium, as we lack experience. 

It is important to keep in mind that, in order to improve the results, it would be useful to look for another data set. This is because the data used is very unbalanced, with the vast majority of the samples being from U.S. workers, which may bias the results. Also, having a dataset with more numerical variables could help us when using other algorithms. In the future, it would also be interesting to use more advanced methods, such as neural networks. 

From a personal point of view as students, we consider that this practice is a good way to finish the course, since it forces you to review all the material and to explore the data in every way you can think of to extract as much information as possible from it. The most difficult points for us have been dealing with the data, since they present several difficulties such as how unbalanced the classes are or having practically only factors, and the fact of presenting the whole study in only eight pages. 


